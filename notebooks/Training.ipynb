{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e8f2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "repo_root = Path(\"D:/Divas/Projects/MSCS/7643/Project/TORTOISE\").resolve()\n",
    "sys.path.insert(0, str(repo_root / \"src\"))\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tortoise.train import get_device, print_device_info, train_one_epoch, evaluate\n",
    "from tortoise.dataloader import build_dataloaders\n",
    "from tortoise.model import U_Net\n",
    "\n",
    "print_device_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce89d725",
   "metadata": {},
   "source": [
    "## Step 1: Setup device and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae957aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get device (automatically selects GPU if available)\n",
    "device = get_device()\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "# Initialize model (13 input channels for MS tiles, 1 output for binary segmentation)\n",
    "model = U_Net(img_ch=13, output_ch=1)\n",
    "model = model.to(device)  # Move model to GPU\n",
    "\n",
    "print(f\"Model moved to {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae6962",
   "metadata": {},
   "source": [
    "## Step 2: Build dataloaders (with pin_memory for GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85385fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloaders\n",
    "# Note: pin_memory is automatically enabled when CUDA is available\n",
    "train_loader, val_loader, test_loader = build_dataloaders(\n",
    "    tiles_dir=repo_root / \"data\" / \"tiles\",\n",
    "    batch_size=8,\n",
    "    seed=42,\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.1,\n",
    "    use_ms=True,          # Use multispectral (13 bands)\n",
    "    use_rgb=False,\n",
    "    num_workers=0,         # Increase on multi-core systems (e.g., 4)\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches:   {len(val_loader)}\")\n",
    "print(f\"Test batches:  {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771c3ba",
   "metadata": {},
   "source": [
    "## Step 3: Verify data is on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c9eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab one batch to verify GPU transfer\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Data is still on CPU at this point (DataLoader default)\n",
    "print(f\"Batch MS shape: {batch['ms'].shape}, device: {batch['ms'].device}\")\n",
    "\n",
    "# Move to GPU (this happens in train_one_epoch, but shown here for clarity)\n",
    "ms_gpu = batch[\"ms\"].to(device)\n",
    "label_gpu = batch[\"label\"].to(device)\n",
    "mask_gpu = batch[\"mask\"].to(device)\n",
    "\n",
    "print(f\"After .to(device):\")\n",
    "print(f\"  MS device:    {ms_gpu.device}\")\n",
    "print(f\"  Label device: {label_gpu.device}\")\n",
    "print(f\"  Mask device:  {mask_gpu.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f0b3d",
   "metadata": {},
   "source": [
    "## Step 4: Training loop with GPU and mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Optional: mixed precision training (faster, uses less GPU memory)\n",
    "scaler = torch.cuda.amp.GradScaler() if device.type == \"cuda\" else None\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_one_epoch(\n",
    "        model=model,\n",
    "        loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        scaler=scaler,  # Enable mixed precision if CUDA available\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = evaluate(\n",
    "        model=model,\n",
    "        loader=val_loader,\n",
    "        desc=\"Val\",\n",
    "        device=device,\n",
    "        use_amp=(scaler is not None),\n",
    "    )\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.6f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.6f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b4c7f",
   "metadata": {},
   "source": [
    "## Step 5: Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca083fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(\n",
    "    model=model,\n",
    "    loader=test_loader,\n",
    "    desc=\"Test\",\n",
    "    device=device,\n",
    "    use_amp=(scaler is not None),\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bfd332",
   "metadata": {},
   "source": [
    "## GPU Memory Management Tips\n",
    "\n",
    "1. **Monitor GPU memory:**\n",
    "   ```python\n",
    "   import torch\n",
    "   print(f\"GPU allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "   print(f\"GPU reserved:  {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "   ```\n",
    "\n",
    "2. **Clear GPU cache** (use sparingly):\n",
    "   ```python\n",
    "   torch.cuda.empty_cache()\n",
    "   ```\n",
    "\n",
    "3. **Enable mixed precision** (AMP) for 2-3x speedup on modern GPUs:\n",
    "   - Already enabled in `train_one_epoch` when `scaler` is passed\n",
    "\n",
    "4. **Increase batch size** if GPU memory allows (speeds up training)\n",
    "\n",
    "5. **Increase num_workers** for faster data loading (set to CPU core count, e.g., 4-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c16de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: monitor GPU memory\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU memory reserved:  {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
